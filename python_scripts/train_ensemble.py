#!/usr/bin/env python3
"""
ØªØ¯Ø±ÙŠØ¨ Ensemble Model (Random Forest + XGBoost + LightGBM)
"""

import sys
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

import os
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, parent_dir)

from backend.data.database import Database
from backend.models.ml_model import StockMLModel

# ØªØ«Ø¨ÙŠØª XGBoost Ùˆ LightGBM Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙˆÙ†Ø§ Ù…ÙˆØ¬ÙˆØ¯ÙŠÙ†
try:
    from xgboost import XGBClassifier
    print("âœ… XGBoost Ù…ØªØ§Ø­")
except ImportError:
    print("âš ï¸  ØªØ«Ø¨ÙŠØª XGBoost...")
    os.system("pip install xgboost -q")
    from xgboost import XGBClassifier

try:
    from lightgbm import LGBMClassifier
    print("âœ… LightGBM Ù…ØªØ§Ø­")
except ImportError:
    print("âš ï¸  ØªØ«Ø¨ÙŠØª LightGBM...")
    os.system("pip install lightgbm -q")
    from lightgbm import LGBMClassifier


def create_target(df):
    """Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù‡Ø¯Ù (Buy/Sell/Hold)"""
    # Ù†Ø¸Ø±Ø© Ù„Ù„Ø£Ù…Ø§Ù… 5 Ø£ÙŠØ§Ù…
    df['future_return'] = df['close'].shift(-5) / df['close'] - 1
    
    # ØªØµÙ†ÙŠÙ
    conditions = [
        df['future_return'] > 0.03,  # Ø§Ø±ØªÙØ§Ø¹ > 3% = Buy
        df['future_return'] < -0.03,  # Ø§Ù†Ø®ÙØ§Ø¶ > 3% = Sell
    ]
    choices = ['buy', 'sell']
    df['target'] = np.select(conditions, choices, default='hold')
    
    return df


def prepare_training_data(db):
    """ØªØ­Ø¶ÙŠØ± Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨"""
    print("ğŸ“Š Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ©...")
    
    # Ø¬Ù„Ø¨ Ø¢Ø®Ø± 500 ÙŠÙˆÙ…
    data = db.get_all_historical_data(days=500)
    
    if not data:
        print("âŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª!")
        return None
    
    df = pd.DataFrame(data)
    print(f"âœ… ØªÙ… Ø¬Ù„Ø¨ {len(df)} Ø³Ø¬Ù„ Ù…Ù† {df['symbol'].nunique()} Ø³Ù‡Ù…")
    
    # Ù…Ø¹Ø§Ù„Ø¬Ø© ÙƒÙ„ Ø³Ù‡Ù…
    ml_model = StockMLModel()
    all_data = []
    
    for symbol in df['symbol'].unique():
        stock_df = df[df['symbol'] == symbol].copy()
        stock_df = stock_df.sort_values('date')
        
        # ØªØ­ÙˆÙŠÙ„ Decimal Ø¥Ù„Ù‰ float
        for col in ['open', 'high', 'low', 'close', 'volume']:
            if col in stock_df.columns:
                stock_df[col] = stock_df[col].astype(float)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª
        stock_df = ml_model.calculate_technical_indicators(stock_df)
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù‡Ø¯Ù
        stock_df = create_target(stock_df)
        
        all_data.append(stock_df)
    
    # Ø¯Ù…Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    final_df = pd.concat(all_data, ignore_index=True)
    final_df = final_df.dropna()
    
    print(f"âœ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©: {len(final_df)} Ø¹ÙŠÙ†Ø©")
    
    return final_df


def train_ensemble(df):
    """ØªØ¯Ø±ÙŠØ¨ Ensemble Model"""
    print("\nğŸ¤– Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ensemble Model...")
    
    # Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„ÙƒØ§Ù…Ù„Ø© (Ù…Ø¹ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©)
    features = [
        # Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        'rsi', 'macd', 'macd_signal', 'macd_diff',
        'sma_20', 'sma_50', 'ema_12',
        'bb_width', 'atr', 'volume_ratio',
        'price_change', 'price_change_5d',
        'stoch_k', 'stoch_d', 'adx', 'obv_ema',
        # Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø´Ù…ÙˆØ¹ Ø§Ù„ÙŠØ§Ø¨Ø§Ù†ÙŠØ©
        'doji', 'hammer', 'shooting_star',
        'bullish_engulfing', 'bearish_engulfing',
        'morning_star', 'evening_star',
        # Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„Ø¯Ø¹Ù… ÙˆØ§Ù„Ù…Ù‚Ø§ÙˆÙ…Ø©
        'dist_from_support', 'dist_from_resistance',
        'sr_position', 'near_support', 'near_resistance'
    ]
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©
    available_features = [f for f in features if f in df.columns]
    print(f"ğŸ“Š Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©: {len(available_features)} Ù…Ù† {len(features)}")
    
    X = df[available_features]
    y = df['target']
    
    # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    print(f"ğŸ“Š Ø§Ù„ØªØ¯Ø±ÙŠØ¨: {len(X_train)} Ø¹ÙŠÙ†Ø©")
    print(f"ğŸ“Š Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±: {len(X_test)} Ø¹ÙŠÙ†Ø©")
    print(f"ğŸ“Š ØªÙˆØ²ÙŠØ¹ Ø§Ù„ÙØ¦Ø§Øª:\n{y_train.value_counts()}")
    
    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ÙØ¦Ø§Øª Ù„Ø£Ø±Ù‚Ø§Ù… Ù„Ù€ XGBoost
    label_map = {'buy': 0, 'hold': 1, 'sell': 2}
    y_train_num = y_train.map(label_map)
    y_test_num = y_test.map(label_map)
    
    # ==================== Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ====================
    
    print("\n" + "=" * 50)
    print("ğŸŒ² ØªØ¯Ø±ÙŠØ¨ Random Forest...")
    rf_model = RandomForestClassifier(
        n_estimators=200,
        max_depth=15,
        min_samples_split=10,
        min_samples_leaf=5,
        class_weight='balanced',
        random_state=42,
        n_jobs=-1
    )
    rf_model.fit(X_train, y_train)
    rf_pred = rf_model.predict(X_test)
    rf_acc = accuracy_score(y_test, rf_pred)
    print(f"âœ… Ø¯Ù‚Ø© Random Forest: {rf_acc*100:.2f}%")
    
    print("\n" + "=" * 50)
    print("ğŸš€ ØªØ¯Ø±ÙŠØ¨ XGBoost...")
    xgb_model = XGBClassifier(
        n_estimators=200,
        max_depth=10,
        learning_rate=0.1,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        n_jobs=-1,
        use_label_encoder=False,
        eval_metric='mlogloss'
    )
    xgb_model.fit(X_train, y_train_num)
    xgb_pred_num = xgb_model.predict(X_test)
    reverse_map = {0: 'buy', 1: 'hold', 2: 'sell'}
    xgb_pred = pd.Series(xgb_pred_num).map(reverse_map)
    xgb_acc = accuracy_score(y_test, xgb_pred)
    print(f"âœ… Ø¯Ù‚Ø© XGBoost: {xgb_acc*100:.2f}%")
    
    print("\n" + "=" * 50)
    print("âš¡ ØªØ¯Ø±ÙŠØ¨ LightGBM...")
    lgb_model = LGBMClassifier(
        n_estimators=200,
        max_depth=10,
        learning_rate=0.1,
        subsample=0.8,
        colsample_bytree=0.8,
        class_weight='balanced',
        random_state=42,
        n_jobs=-1,
        verbose=-1
    )
    lgb_model.fit(X_train, y_train)
    lgb_pred = lgb_model.predict(X_test)
    lgb_acc = accuracy_score(y_test, lgb_pred)
    print(f"âœ… Ø¯Ù‚Ø© LightGBM: {lgb_acc*100:.2f}%")
    
    # ==================== Ensemble (Voting) ====================
    
    print("\n" + "=" * 50)
    print("ğŸ¯ Ø¨Ù†Ø§Ø¡ Ensemble Model (Voting)...")
    
    # Soft Voting - ÙŠØ£Ø®Ø° Ù…ØªÙˆØ³Ø· Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª
    ensemble_model = VotingClassifier(
        estimators=[
            ('rf', rf_model),
            ('lgb', lgb_model)
        ],
        voting='soft',
        weights=[1, 1]  # ÙˆØ²Ù† Ù…ØªØ³Ø§ÙˆÙŠ
    )
    
    ensemble_model.fit(X_train, y_train)
    ensemble_pred = ensemble_model.predict(X_test)
    ensemble_acc = accuracy_score(y_test, ensemble_pred)
    
    print(f"\nâœ… Ø¯Ù‚Ø© Ensemble Model: {ensemble_acc*100:.2f}%")
    
    # ==================== Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© ====================
    
    print("\n" + "=" * 50)
    print("ğŸ“Š Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬:")
    print("=" * 50)
    print(f"  ğŸŒ² Random Forest: {rf_acc*100:.2f}%")
    print(f"  ğŸš€ XGBoost:       {xgb_acc*100:.2f}%")
    print(f"  âš¡ LightGBM:      {lgb_acc*100:.2f}%")
    print(f"  ğŸ¯ Ensemble:      {ensemble_acc*100:.2f}%")
    
    # Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬
    models = {
        'rf': (rf_model, rf_acc),
        'xgb': (xgb_model, xgb_acc),
        'lgb': (lgb_model, lgb_acc),
        'ensemble': (ensemble_model, ensemble_acc)
    }
    
    best_name = max(models, key=lambda x: models[x][1])
    best_model, best_acc = models[best_name]
    
    print(f"\nğŸ† Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬: {best_name.upper()} ({best_acc*100:.2f}%)")
    
    # ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØµÙ†ÙŠÙ Ù„Ù„Ø£ÙØ¶Ù„
    if best_name == 'xgb':
        best_pred = xgb_pred
    elif best_name == 'rf':
        best_pred = rf_pred
    elif best_name == 'lgb':
        best_pred = lgb_pred
    else:
        best_pred = ensemble_pred
    
    print("\nğŸ“Š ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØµÙ†ÙŠÙ (Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬):")
    print(classification_report(y_test, best_pred))
    
    # Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ù…ÙŠØ²Ø§Øª
    print("\nğŸ“Š Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ù…ÙŠØ²Ø§Øª (Random Forest):")
    feature_importance = pd.DataFrame({
        'feature': available_features,
        'importance': rf_model.feature_importances_
    }).sort_values('importance', ascending=False).head(15)
    print(feature_importance.to_string(index=False))
    
    # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ensemble ÙƒÙ†Ù…ÙˆØ°Ø¬ Ù†Ù‡Ø§Ø¦ÙŠ
    return ensemble_model, available_features, {
        'rf_acc': rf_acc,
        'xgb_acc': xgb_acc,
        'lgb_acc': lgb_acc,
        'ensemble_acc': ensemble_acc
    }


def main():
    print("=" * 70)
    print("ğŸš€ ØªØ¯Ø±ÙŠØ¨ Ensemble Model (RF + XGBoost + LightGBM)")
    print("=" * 70)
    
    db = Database()
    
    try:
        # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        df = prepare_training_data(db)
        
        if df is None or len(df) < 1000:
            print("âŒ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± ÙƒØ§ÙÙŠØ©!")
            return
        
        # ØªØ¯Ø±ÙŠØ¨
        model, features, accuracies = train_ensemble(df)
        
        # Ø­ÙØ¸
        ml_model = StockMLModel()
        ml_model.model = model
        ml_model.features = features
        ml_model.save_model()
        
        print("\n" + "=" * 70)
        print("âœ… Ø§ÙƒØªÙ…Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ù†Ø¬Ø§Ø­!")
        print(f"ğŸ“Š Ø§Ù„Ø¯Ù‚Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©: {accuracies['ensemble_acc']*100:.2f}%")
        print("=" * 70)
        
    except Exception as e:
        print(f"\nâŒ Ø®Ø·Ø£: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        db.close()


if __name__ == "__main__":
    main()
